# Python 网络爬虫与正则表达式笔记

## 一、网页爬虫基础

### 1. 两种主要爬虫方法

- **静态爬取**：使用 requests 库直接读取网页源代码
- **动态爬取**：使用 selenium 控制浏览器读取网页

### 2. requests 库常见问题及解决方案

#### 2.1 编码问题（乱码）

```python
import requests
url = "https://www.example.com"
res = requests.get(url)
# 查看并修改编码
print(res.encoding)  # 查看当前编码
res.encoding = 'utf-8'  # 设置正确的编码
# 或者直接转换
text = res.text.encode('iso-8859-1').decode('utf-8')
```

#### 2.2 伪装浏览器（反反爬虫）

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/87.0'
}
res = requests.get(url, headers=headers)
```

#### 2.3 请求超时处理

```python
try:
    res = requests.get(url, headers=headers, timeout=10)
except:
    print("请求超时")

# 更健壮的方式：循环尝试
ind_not_get_data = True
while ind_not_get_data:
    try:
        res = requests.get(url, headers=headers, timeout=10)
        ind_not_get_data = False
    except:
        print("请求超时，重试中...")
```

#### 2.4 设置代理 IP

```python
proxy = '219.220.111.3:25764'  # 需自行购买或获取
proxies = {
    'http': f'http://{proxy}',
    'https': f'https://{proxy}'
}
res = requests.get(url, headers=headers, proxies=proxies, timeout=10)
```

## 二、正则表达式（re 模块）

### 1. 正则表达式基本概念

正则表达式是一种描述字符串模式的工具，可以用来进行文本搜索、替换等操作。

### 2. 元字符及其含义

| 元字符 | 含义                         |
| ------ | ---------------------------- |
| .      | 匹配除换行符以外的任意字符   |
| \w     | 匹配字母、数字、下划线或汉字 |
| \s     | 匹配任意空白符               |
| \d     | 匹配数字                     |
| \b     | 匹配单词边界位置             |
| ^      | 匹配字符串开始位置           |
| $      | 匹配字符串结束位置           |

### 3. 重复限定符

| 限定符 | 含义              |
| ------ | ----------------- |
| \*     | 重复零次或更多次  |
| +      | 重复一次或更多次  |
| ?      | 重复零次或一次    |
| {n}    | 重复 n 次         |
| {n,}   | 重复 n 次或更多次 |
| {n,m}  | 重复 n 到 m 次    |

### 4. 数学表示

假设有字符串 $S = \{s_1, s_2, ..., s_n\}$ 和正则表达式模式 $P$，则匹配过程可表示为:

$$Match(P, S) \rightarrow \{S' \subset S | S' \text{ 满足模式 } P\}$$

对于贪婪匹配，选择满足模式的最长子串:

$$S'_{greedy} = \max_{length}(S' \subset S | S' \text{ 满足模式 } P)$$

对于惰性匹配，选择满足模式的最短子串:

$$S'_{lazy} = \min_{length}(S' \subset S | S' \text{ 满足模式 } P)$$

### 5. re 模块主要函数

```python
import re

# 1. 查找单个匹配
match_obj = re.search(pattern, string)  # 任意位置匹配
match_obj = re.match(pattern, string)   # 从开头匹配
match_obj = re.fullmatch(pattern, string)  # 完全匹配

# 2. 查找所有匹配
all_matches = re.findall(pattern, string)  # 返回列表
iterator = re.finditer(pattern, string)    # 返回迭代器

# 3. 分割字符串
parts = re.split(pattern, string, maxsplit=0)

# 4. 替换
new_string = re.sub(pattern, replacement, string, count=0)
# 计数替换
result_tuple = re.subn(pattern, replacement, string, count=0)

# 5. 编译正则表达式对象（提高效率）
pattern_obj = re.compile(pattern)
result = pattern_obj.search(string)
```

### 6. 常见正则表达式示例

#### 匹配电话号码

```python
pattern = r"\(?0\d{2}[)-]?\d{8}"
text = "联系方式：(010)88886666，022-22334455"
phones = re.findall(pattern, text)
print(phones)  # ['(010)88886666', '022-22334455']
```

#### 匹配邮箱

```python
pattern = r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
text = "联系邮箱：example@gmail.com 和 test.user@company.cn"
emails = re.findall(pattern, text)
print(emails)  # ['example@gmail.com', 'test.user@company.cn']
```

### 7. 分组捕获

使用括号 `()` 可以捕获匹配的子字符串：

```python
pattern = r"(\w+) (\w+)"
text = "金融计算 教师"
m = re.match(pattern, text)
print(m.group(0))  # 整个匹配: 金融计算 教师
print(m.group(1))  # 第一个捕获组: 金融计算
print(m.group(2))  # 第二个捕获组: 教师
print(m.groups())  # 所有捕获组: ('金融计算', '教师')
```

### 8. 计算示例：提取所有数字并求和

```python
text = "收入为2500元，支出为1200元，余额为1300元"
pattern = r"\d+"
numbers = re.findall(pattern, text)
numbers = [int(num) for num in numbers]
total = sum(numbers)
print(f"总计: {total}")  # 输出: 总计: 5000
```

## 三、网络爬虫综合实例

### 1. 基本网页爬取示例

```python
import requests
import re
import time

def fetch_data(url):
    # 健壮的数据获取函数
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/87.0'
    }
    retry_count = 0
    while retry_count < 3:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            return response.text
        except:
            print(f"获取失败，第{retry_count+1}次重试...")
            retry_count += 1
            time.sleep(2)
    return None

# 示例：获取百度搜索结果
keyword = "贵州茅台"
url = f"https://www.baidu.com/s?wd={keyword}"
html = fetch_data(url)

if html:
    # 提取搜索结果标题
    pattern = r'<h3 class=".*?"><a.*?>(.*?)</a></h3>'
    titles = re.findall(pattern, html, re.S)
    for i, title in enumerate(titles[:5], 1):
        # 清理HTML标签
        clean_title = re.sub(r'<.*?>', '', title)
        print(f"{i}. {clean_title}")
```

### 2. 简易爬虫数据分析

```python
import requests
import re
import numpy as np

# 爬取某网站股票价格
url = "https://example.com/stock"
html = requests.get(url).text

# 提取价格数据
pattern = r'股价：(\d+\.\d+)'
prices = re.findall(pattern, html)
prices = [float(price) for price in prices]

# 简单统计分析
if prices:
    avg_price = np.mean(prices)
    max_price = np.max(prices)
    min_price = np.min(prices)
    std_dev = np.std(prices)

    print(f"平均价格: {avg_price:.2f}")
    print(f"最高价格: {max_price:.2f}")
    print(f"最低价格: {min_price:.2f}")
    print(f"标准差: {std_dev:.2f}")

    # 计算波动率
    volatility = std_dev / avg_price * 100
    print(f"波动率: {volatility:.2f}%")
```
