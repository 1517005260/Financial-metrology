{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1899e48",
   "metadata": {},
   "source": [
    "# 实验六：Python网络爬虫实战\n",
    "\n",
    "1.\t认真阅读三篇文献资料，了解互联网开源数据和证券市场的相关研究，重点关注其中的研究思路和分析方法。\n",
    "\n",
    "2.\t利用所给Data_600618中的数据文件，结合正则表达式提取出股票600618的日度数据（日期，开盘，最高，最低，收盘，成交量，成交金额），并输出至csv文件，在实验报告中给出数据截图。\n",
    "\n",
    "3.\t在东方股吧下载“长久物流吧”（`https://guba.eastmoney.com/list,603569.html`）所有发帖数据（建议以班级为单位组队分工爬取，每人爬十几页），提取出帖子作者，发帖时间，阅读量，评论数，帖子标题，帖子链接，并将结果输出至文本文件“data_guba_cjwl.txt”。\n",
    "\n",
    "4.\t利用新浪数据接口读取长久物流的历史日度数据，接口数据链接： \n",
    "`http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData?symbol=sh603569&scale=240&ma=no&datalen=10000`\n",
    "从返回的数据中提取，交易日，开盘，最高，最低，收盘，成交量数据，并将结果输出至文本文件“data_sina_cjwl.txt”。\n",
    "\n",
    "5.\t构建股吧信息量指标（根据发帖时间、阅读量、评论数、帖子标题自行设计指标），将该指标作为预测因子，检验其对长久物流的超额收益率是否具有可预测性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470575e1",
   "metadata": {},
   "source": [
    "## 正则表达式提取 600618 的日度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463307cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746c8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_2009_2013(html):\n",
    "    data = []\n",
    "    table = re.search(r'<table class=\"table_bg001 border_box limit_sale\">[\\s\\S]*?</table>', html)\n",
    "    if not table:\n",
    "        return []\n",
    "    rows = re.findall(r'<tr class=[\\'\"]?(?:dbrow|)[\\'\"]?><td>[\\s\\S]*?</tr>', table.group(0))\n",
    "    for row in rows:\n",
    "        try:\n",
    "            cells = re.findall(r'<td[^>]*>([\\s\\S]*?)</td>', row)\n",
    "            if len(cells) < 11:\n",
    "                continue\n",
    "            data.append({\n",
    "                'date': cells[0],\n",
    "                'open': float(cells[1]),\n",
    "                'high': float(cells[2]),\n",
    "                'low': float(cells[3]),\n",
    "                'close': float(cells[4]),\n",
    "                'change': float(cells[5]),\n",
    "                'change_percent': float(cells[6]),\n",
    "                'volume': int(cells[7].replace(',', '')),\n",
    "                'amount': float(cells[8].replace(',', '')) * 10000,\n",
    "                'amplitude': float(cells[9]),\n",
    "                'turnover_rate': float(cells[10])\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "def extract_data_before_2014(html):\n",
    "    if 'table_bg001 border_box limit_sale' in html:\n",
    "        return extract_data_2009_2013(html)\n",
    "\n",
    "    def parse_common(pattern):\n",
    "        return [{\n",
    "            'date': m[0],\n",
    "            'open': float(m[1]),\n",
    "            'high': float(m[2]),\n",
    "            'close': float(m[3]),\n",
    "            'low': float(m[4]),\n",
    "            'volume': int(m[5]),\n",
    "            'amount': int(m[6])\n",
    "        } for m in re.findall(pattern, html, re.DOTALL)]\n",
    "\n",
    "    if \"[考试顺利]\" in html:\n",
    "        if \"href=\" in html or \"target='_blank'\" in html:\n",
    "            pattern_2007 = r'<tr[^>]*>\\s*<td><div[^>]*>\\[考试顺利\\]\\s*(?:<a[^>]*>)?\\s*([\\d-]+)\\s*(?:</a>)?[^<]*</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\](\\d+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\](\\d+)</div></td>'\n",
    "            return parse_common(pattern_2007)\n",
    "        else:\n",
    "            pattern_2004 = r'<tr[^>]*>\\s*<td><div[^>]*>\\[考试顺利\\]\\s*([\\d-]+)\\s*</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\]([\\d.]+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\](\\d+)</div></td>\\s*' \\\n",
    "                           r'<td[^>]*><div[^>]*>\\[考试顺利\\](\\d+)</div></td>'\n",
    "            return parse_common(pattern_2004)\n",
    "    else:\n",
    "        pattern_1999 = r'<tr\\s+(?:class=\"tr_2\"|)>\\s*<td><div align=\"center\">\\s*([\\d-]+)\\s*</div></td>\\s*' \\\n",
    "                       r'<td><div align=\"center\">([\\d.]+)</div></td>\\s*' \\\n",
    "                       r'<td><div align=\"center\">([\\d.]+)</div></td>\\s*' \\\n",
    "                       r'<td><div align=\"center\">([\\d.]+)</div></td>\\s*' \\\n",
    "                       r'<td[^>]*><div align=\"center\">([\\d.]+)</div></td>\\s*' \\\n",
    "                       r'<td[^>]*><div align=\"center\">(\\d+)</div></td>\\s*' \\\n",
    "                       r'<td[^>]*><div align=\"center\">(\\d+)</div></td>'\n",
    "        return parse_common(pattern_1999)\n",
    "\n",
    "def extract_data_after_2014(js):\n",
    "    match = re.search(r'historySearchHandler\\((.*)\\)', js)\n",
    "    if not match:\n",
    "        return []\n",
    "    try:\n",
    "        json_data = json.loads(match.group(1))\n",
    "        hq = json_data[0].get('hq', []) if json_data and isinstance(json_data, list) else []\n",
    "        return [{\n",
    "            'date': i[0],\n",
    "            'open': float(i[1]),\n",
    "            'high': float(i[6]),\n",
    "            'low': float(i[5]),\n",
    "            'close': float(i[2]),\n",
    "            'volume': int(i[7]) * 100,\n",
    "            'amount': float(i[8].replace(',', '')) * 10000 if isinstance(i[8], str) else float(i[8]) * 10000\n",
    "        } for i in hq]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e9a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    处理单个文件，提取股票数据\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # 根据文件名判断数据格式\n",
    "    if any(year in file_path for year in ['2014', '2015', '2016', '2017', '2018', '2019']):\n",
    "        return extract_data_after_2014(content)\n",
    "    else:\n",
    "        return extract_data_before_2014(content)\n",
    "\n",
    "def process_pipeline():\n",
    "    # 数据文件所在的目录\n",
    "    data_dir = './assets/data/Data_600618'  \n",
    "\n",
    "    # 查找所有相关的数据文件\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.startswith('DataHTML_600618_Year_') and filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            file_data = process_file(file_path)\n",
    "            all_data.extend(file_data)\n",
    "    \n",
    "    print(f\"总共提取了 {len(all_data)} 条记录\")\n",
    "    \n",
    "    # 按日期排序\n",
    "    all_data.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    # 输出到CSV文件\n",
    "    output_file = './assets/data/股票600618日度数据.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # 写入表头\n",
    "        writer.writerow(['日期', '开盘价', '最高价', '最低价', '收盘价', '成交量(股)', '成交金额(元)'])\n",
    "        # 写入数据\n",
    "        for item in all_data:\n",
    "            writer.writerow([\n",
    "                item['date'],\n",
    "                item['open'],\n",
    "                item['high'],\n",
    "                item['low'],\n",
    "                item['close'],\n",
    "                item['volume'],\n",
    "                item['amount']\n",
    "            ])\n",
    "    \n",
    "    print(f\"数据已成功提取并保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1931089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共提取了 4775 条记录\n",
      "数据已成功提取并保存到 ./assets/data/股票600618日度数据.csv\n"
     ]
    }
   ],
   "source": [
    "process_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56440472",
   "metadata": {},
   "source": [
    "## 长久物流吧 数据爬取\n",
    "\n",
    "这里很容易被封ip（只能爬取到方正证券的数据），建议采用代理解决，快代理可以免费使用一天。\n",
    "\n",
    "由于后续还要处理爬到的数据，为了处理方便，这里改txt为csv。\n",
    "\n",
    "股吧的帖子详情界面才能有带年份的时间，否则只有月份和日期，下面这份代码为了爬取带年份的，对每个详情页都进行了抓取，因此运行较慢，实际体验约10多个小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3494b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "STOCK_ID = \"603569\"  # 长久物流的股票代码\n",
    "STOCK_NAME = \"长久物流\"  # 股票名称，用于验证\n",
    "BASE_URL = f\"https://guba.eastmoney.com/list,{STOCK_ID},f_{{page}}.html\"\n",
    "OUTPUT_DIR = \"./assets/data\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"data_guba_cjwl.csv\")\n",
    "MAX_PAGES = 1000  # 设置一个足够大的值爬取所有页面\n",
    "MAX_THREADS = 5  # 详情页爬取的最大线程数\n",
    "\n",
    "# 代理设置\n",
    "PROXY_USER = \"t14428474946629\"\n",
    "PROXY_PASS = \"k0fxm4s0\"\n",
    "PROXY_HOST = \"x368.kdltpspro.com:15818\"\n",
    "\n",
    "# 请求计数器(每3次强制切换IP)\n",
    "request_counter = 0\n",
    "# 锁对象 - 用于多线程环境下的文件写入\n",
    "file_lock = None\n",
    "\n",
    "def get_proxies():\n",
    "    \"\"\"获取代理配置\"\"\"\n",
    "    return {\n",
    "        \"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}\",\n",
    "        \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}\"\n",
    "    }\n",
    "\n",
    "def get_headers():\n",
    "    \"\"\"获取随机UA\"\"\"\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        user_agent = ua.random\n",
    "    except:\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
    "        ]\n",
    "        user_agent = random.choice(user_agents)\n",
    "    \n",
    "    return {\n",
    "        'User-Agent': user_agent,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        'Connection': 'close',  # 非常重要，确保不复用连接，这样可以获取新IP\n",
    "        'Cache-Control': 'no-cache'\n",
    "    }\n",
    "\n",
    "def force_new_ip():\n",
    "    \"\"\"强制获取新IP - 通过简单的请求测试站点来切换\"\"\"\n",
    "    global request_counter\n",
    "    request_counter = 0  # 重置计数器\n",
    "    \n",
    "    try:\n",
    "        # 对测试站点发起请求以更换IP\n",
    "        test_url = \"https://dev.kdlapi.com/testproxy\"\n",
    "        proxies = get_proxies()\n",
    "        headers = get_headers()\n",
    "        \n",
    "        print(\"正在切换IP...\")\n",
    "        response = requests.get(test_url, proxies=proxies, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"IP已切换\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"切换IP时出错: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_page(url, retry_limit=3):\n",
    "    \"\"\"获取页面内容\"\"\"\n",
    "    global request_counter\n",
    "    \n",
    "    # 每3次请求强制更换IP\n",
    "    request_counter += 1\n",
    "    if request_counter >= 3:\n",
    "        force_new_ip()\n",
    "    \n",
    "    proxies = get_proxies()\n",
    "    headers = get_headers()\n",
    "    \n",
    "    retry_count = 0\n",
    "    while retry_count < retry_limit:\n",
    "        try:\n",
    "            # 创建新的session以确保不复用连接\n",
    "            session = requests.Session()\n",
    "            session.keep_alive = False\n",
    "            \n",
    "            # 添加随机延迟\n",
    "            time.sleep(random.uniform(3, 7))\n",
    "            \n",
    "            # 发送请求\n",
    "            response = session.get(url, proxies=proxies, headers=headers, timeout=15)\n",
    "            response.encoding = 'utf-8'\n",
    "            \n",
    "            # 检查是否被重定向到其他股票\n",
    "            if STOCK_NAME not in response.text and (\"方正证券\" in response.text or \"验证码\" in response.text):\n",
    "                print(f\"检测到被重定向或需要验证码，尝试切换IP...\")\n",
    "                force_new_ip()\n",
    "                retry_count += 1\n",
    "                continue\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"获取页面失败: {e}\")\n",
    "            retry_count += 1\n",
    "            force_new_ip()  # 切换IP\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_page(html, page_num):\n",
    "    \"\"\"解析页面内容提取帖子列表\"\"\"\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    items = []\n",
    "    \n",
    "    # 检查是否是目标股票页面\n",
    "    title = soup.title.text if soup.title else \"\"\n",
    "    if STOCK_NAME not in title:\n",
    "        print(f\"警告: 页面标题不含目标股票名: {title}\")\n",
    "        return []\n",
    "    \n",
    "    # 提取帖子列表\n",
    "    post_items = soup.select('div.articleh, div.articleh.odd')\n",
    "    if not post_items:\n",
    "        post_items = soup.select('div[class*=\"articleh\"]')\n",
    "    \n",
    "    if not post_items:\n",
    "        post_items = []\n",
    "        rows = soup.select('table tr')\n",
    "        for row in rows:\n",
    "            if row.select('th') or (row.get('class') and 'listhead' in row.get('class')):\n",
    "                continue\n",
    "            if row.select('a'):\n",
    "                post_items.append(row)\n",
    "    \n",
    "    print(f\"在第 {page_num} 页找到 {len(post_items)} 个帖子\")\n",
    "    \n",
    "    # 解析每一个帖子\n",
    "    for item in post_items:\n",
    "        try:\n",
    "            # 根据元素类型解析\n",
    "            if item.name == 'div':  # div结构\n",
    "                read_count = item.select_one('span.l1').text.strip() if item.select_one('span.l1') else \"0\"\n",
    "                comment_count = item.select_one('span.l2').text.strip() if item.select_one('span.l2') else \"0\"\n",
    "                \n",
    "                title_elem = item.select_one('span.l3 > a')\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                title = title_elem.text.strip()\n",
    "                link = title_elem.get('href', '')\n",
    "                \n",
    "                author = item.select_one('span.l4 > a').text.strip() if item.select_one('span.l4 > a') else \"未知作者\"\n",
    "                \n",
    "                # 列表页的时间信息可能不全，先获取初步时间，后续从详情页更新\n",
    "                initial_time = item.select_one('span.l5').text.strip() if item.select_one('span.l5') else \"\"\n",
    "                \n",
    "            elif item.name == 'tr':  # 表格结构\n",
    "                cells = item.select('td')\n",
    "                if len(cells) < 4:\n",
    "                    continue\n",
    "                \n",
    "                read_count = cells[0].text.strip()\n",
    "                comment_count = cells[1].text.strip()\n",
    "                \n",
    "                title_link = cells[2].select_one('a')\n",
    "                if not title_link:\n",
    "                    continue\n",
    "                title = title_link.text.strip()\n",
    "                link = title_link.get('href', '')\n",
    "                \n",
    "                author = cells[3].text.strip() if len(cells) > 3 else \"未知作者\"\n",
    "                initial_time = cells[4].text.strip() if len(cells) > 4 else \"\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # 验证链接中的股票代码\n",
    "            if '/news,' in link and STOCK_ID not in link:\n",
    "                print(f\"警告: 发现其他股票代码的帖子链接: {link}\")\n",
    "                continue\n",
    "                \n",
    "            # 处理相对链接\n",
    "            if link.startswith('/'):\n",
    "                link = f\"https://guba.eastmoney.com{link}\"\n",
    "            elif not link.startswith('http'):\n",
    "                link = f\"https://guba.eastmoney.com/{link}\"\n",
    "            \n",
    "            # 添加到结果列表\n",
    "            items.append({\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'post_time': initial_time,  # 初步时间，后续会从详情页获取完整时间\n",
    "                'read_count': read_count,\n",
    "                'comment_count': comment_count,\n",
    "                'url': link\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析帖子时出错: {e}\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "def get_detail_time(post_url):\n",
    "    \"\"\"从帖子详情页获取准确的发帖时间\"\"\"\n",
    "    html = get_page(post_url)\n",
    "    if not html:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # 尝试多种选择器匹配时间元素\n",
    "    time_selectors = [\n",
    "        'div.time', \n",
    "        'div.author-info div.time', \n",
    "        'div.author-info time', \n",
    "        'div.zwfbtime',\n",
    "        'div.zwfbtime span',\n",
    "        'div[class*=\"time\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in time_selectors:\n",
    "        time_elem = soup.select_one(selector)\n",
    "        if time_elem:\n",
    "            time_text = time_elem.text.strip()\n",
    "            # 尝试提取日期时间格式 (例如: 2016-07-29 20:56:39)\n",
    "            date_pattern = r'(\\d{4}-\\d{1,2}-\\d{1,2}\\s+\\d{1,2}:\\d{1,2}:\\d{1,2})'\n",
    "            match = re.search(date_pattern, time_text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "    \n",
    "    # 如果上面的选择器都失败了，尝试查找任何时间格式的文本\n",
    "    date_pattern = r'(\\d{4}-\\d{1,2}-\\d{1,2}\\s+\\d{1,2}:\\d{1,2}(?::\\d{1,2})?)'\n",
    "    for element in soup.find_all(['div', 'span']):\n",
    "        text = element.text.strip()\n",
    "        match = re.search(date_pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_post_detail(post):\n",
    "    \"\"\"处理单个帖子的详情信息\"\"\"\n",
    "    post_url = post['url']\n",
    "    print(f\"正在获取详情: {post['title'][:10]}...\")\n",
    "    \n",
    "    # 获取详情页的精确时间\n",
    "    detail_time = get_detail_time(post_url)\n",
    "    if detail_time:\n",
    "        post['post_time'] = detail_time\n",
    "    \n",
    "    # 将处理后的帖子写入CSV文件\n",
    "    with file_lock:\n",
    "        with open(OUTPUT_FILE, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                post['title'],\n",
    "                post['author'],\n",
    "                post['post_time'],\n",
    "                post['read_count'],\n",
    "                post['comment_count'],\n",
    "                post['url']\n",
    "            ])\n",
    "    \n",
    "    return post\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    global file_lock\n",
    "    from threading import Lock\n",
    "    file_lock = Lock()\n",
    "    \n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        # 创建CSV文件并写入表头\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"帖子标题\", \"作者\", \"发帖时间\", \"阅读量\", \"评论数\", \"帖子链接\"])\n",
    "        \n",
    "        print(f\"开始爬取长久物流吧 (股票代码: {STOCK_ID})...\")\n",
    "        print(f\"数据将保存到: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "        \n",
    "        total_items = 0\n",
    "        page = 1\n",
    "        empty_count = 0\n",
    "        max_empty = 3\n",
    "        \n",
    "        # 初始强制切换IP\n",
    "        force_new_ip()\n",
    "        \n",
    "        # 创建线程池\n",
    "        executor = ThreadPoolExecutor(max_workers=MAX_THREADS)\n",
    "        future_to_post = {}\n",
    "        \n",
    "        # 爬取循环\n",
    "        while page <= MAX_PAGES and empty_count < max_empty:\n",
    "            url = BASE_URL.format(page=page)\n",
    "            print(f\"正在爬取第 {page} 页...\")\n",
    "            \n",
    "            # 获取页面内容\n",
    "            html = get_page(url)\n",
    "            \n",
    "            if html is None:\n",
    "                print(f\"无法获取第 {page} 页内容，跳过...\")\n",
    "                empty_count += 1\n",
    "                if empty_count >= max_empty:\n",
    "                    print(\"连续多页获取失败，停止爬取\")\n",
    "                    break\n",
    "                page += 1\n",
    "                continue\n",
    "            \n",
    "            # 解析页面获取帖子\n",
    "            items = parse_page(html, page)\n",
    "            \n",
    "            if items:\n",
    "                total_items += len(items)\n",
    "                print(f\"第 {page} 页解析完成，找到 {len(items)} 条帖子，正在获取详情...\")\n",
    "                \n",
    "                # 提交详情页处理任务到线程池\n",
    "                for post in items:\n",
    "                    future = executor.submit(process_post_detail, post)\n",
    "                    future_to_post[future] = post['title']\n",
    "                \n",
    "                # 每5页等待所有任务完成，避免并发过多\n",
    "                if page % 5 == 0:\n",
    "                    for future in future_to_post:\n",
    "                        try:\n",
    "                            future.result()\n",
    "                        except Exception as e:\n",
    "                            post_title = future_to_post[future]\n",
    "                            print(f\"处理帖子失败: {post_title[:15]}..., 错误: {e}\")\n",
    "                    \n",
    "                    # 清空任务字典\n",
    "                    future_to_post = {}\n",
    "                \n",
    "                empty_count = 0  # 成功获取数据，重置计数器\n",
    "            else:\n",
    "                print(f\"第 {page} 页未解析到任何帖子\")\n",
    "                empty_count += 1\n",
    "                if empty_count >= max_empty:\n",
    "                    print(\"连续多页未能解析到帖子，停止爬取\")\n",
    "                    break\n",
    "            \n",
    "            # 进入下一页\n",
    "            page += 1\n",
    "        \n",
    "        # 等待所有剩余任务完成\n",
    "        print(\"等待所有详情页处理完成...\")\n",
    "        for future in future_to_post:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                post_title = future_to_post[future]\n",
    "                print(f\"处理帖子失败: {post_title[:15]}..., 错误: {e}\")\n",
    "        \n",
    "        # 关闭线程池\n",
    "        executor.shutdown()\n",
    "        \n",
    "        # 完成消息\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"爬虫完成! 共爬取 {total_items} 条帖子\")\n",
    "        print(f\"数据已保存至: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n错误: 爬虫运行时出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4905d",
   "metadata": {},
   "source": [
    "## 新浪数据接口爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce10c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(stock_code):\n",
    "    \"\"\"\n",
    "    从新浪财经获取股票历史数据\n",
    "    \n",
    "    参数:\n",
    "        stock_code: 股票代码，例如 'sh603569'\n",
    "    \n",
    "    返回:\n",
    "        列表形式的股票数据\n",
    "    \"\"\"\n",
    "    url = f\"http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData?symbol={stock_code}&scale=240&ma=no&datalen=10000\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # 确保请求成功\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 新浪返回的是非标准JSON，需要进行转换\n",
    "        # 将单引号替换为双引号，便于解析\n",
    "        data_str = response.text.replace(\"'\", '\"')\n",
    "        stock_data = json.loads(data_str)\n",
    "        \n",
    "        print(f\"成功获取 {stock_code} 的历史数据，共 {len(stock_data)} 条记录\")\n",
    "        return stock_data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"获取数据时出错: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"解析JSON数据时出错: {e}\")\n",
    "        print(f\"返回的数据: {response.text[:200]}...\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e12a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功获取 sh603569 的历史数据，共 2104 条记录\n",
      "数据已成功保存至 ./assets/data/data_sina_cjwl.txt\n"
     ]
    }
   ],
   "source": [
    "# 长久物流的股票代码\n",
    "stock_code = \"sh603569\"\n",
    "# 指定保存路径\n",
    "output_file = \"./assets/data/data_sina_cjwl.txt\"\n",
    "\n",
    "# 获取股票数据\n",
    "stock_data = fetch_stock_data(stock_code)\n",
    "\n",
    "if stock_data:\n",
    "    # 按照要求的格式提取需要的字段\n",
    "    formatted_data = []\n",
    "    for item in stock_data:\n",
    "        formatted_item = {\n",
    "            \"day\": item[\"day\"],\n",
    "            \"open\": item[\"open\"],\n",
    "            \"high\": item[\"high\"],\n",
    "            \"low\": item[\"low\"],\n",
    "            \"close\": item[\"close\"],\n",
    "            \"volume\": item[\"volume\"]\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"数据已成功保存至 {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0610b5",
   "metadata": {},
   "source": [
    "## 股吧信息量指标预测收益率\n",
    "\n",
    "首先我们先要找到长久物流的收益率，市场收益率选用上证指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dabff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
